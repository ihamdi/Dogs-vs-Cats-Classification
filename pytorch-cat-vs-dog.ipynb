{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import zipfile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing model parameters\n",
    "\n",
    "competition = \"dogs-vs-cats\"\n",
    "\n",
    "epochs = input(\"Number of Epochs? [5]\")\n",
    "if epochs.isdigit()==True and epochs.isdigit()!=0:\n",
    "    epochs = int(epochs)\n",
    "else:\n",
    "    epochs = 5\n",
    "print(\"Number of epochs set to:\",epochs)\n",
    "\n",
    "drop_rate = input(\"Dropout rate? [0%]\")\n",
    "if drop_rate.isdigit()==True:\n",
    "    drop_rate = int(drop_rate)/100\n",
    "else:\n",
    "    drop_rate = 0\n",
    "print(\"Dropout rate set to:\", drop_rate*100,\"%\")\n",
    "\n",
    "batch_size = input(\"Batch size? [96]\")\n",
    "if batch_size.isdigit()==True and int(batch_size)!=0:\n",
    "    batch_size = int(batch_size)\n",
    "else:\n",
    "    batch_size = 96\n",
    "print(\"Batch size set to:\", batch_size)\n",
    "\n",
    "num_workers = input(\"Number of workers? [10]\")\n",
    "if num_workers.isdigit()==True and int(num_workers)<0==False:\n",
    "    num_workers = int(num_workers)\n",
    "else:\n",
    "    num_workers = 10\n",
    "print(\"Number of workers set to:\", num_workers)\n",
    "num_workers\n",
    "\n",
    "lr = input('Learning rate? [0.002]')\n",
    "if lr!='0' and lr.replace('.','',1).isdigit()==True: lr = float(lr)\n",
    "else: lr = 2e-3\n",
    "print(\"Learning rate set to:\",lr)\n",
    "\n",
    "\n",
    "# Choosing dataset parameters\n",
    "\n",
    "download_path = input(\"Download path for dataset zip file? [./]\")\n",
    "if os.path.isdir(download_path)==False:\n",
    "    download_path = \"./\"\n",
    "print(\"Download path set to:\",download_path)\n",
    "\n",
    "dataset_size = input('Size of dataset? [25000]')\n",
    "if dataset_size.isdigit() and int(dataset_size)>0 and int(dataset_size)<=25000:\n",
    "    dataset_size=int(dataset_size)\n",
    "else: dataset_size=25000\n",
    "print('Dataset size set to:',dataset_size)\n",
    "\n",
    "train_percent = input('% of data used for training? [70]')\n",
    "if train_percent.replace('.','',1).isdigit()== True and train_percent!='0': train_percent=int(train_percent)/100\n",
    "else: train_percent = 0.7\n",
    "valid_percent = input('% of data used for validation? [20]')\n",
    "if valid_percent.replace('.','',1).isdigit()== True and valid_percent!='0': valid_percent=int(valid_percent)/100\n",
    "else: valid_percent = 0.2\n",
    "test_percent = 1-train_percent-valid_percent\n",
    "\n",
    "train_val_test_split = (train_percent,valid_percent,test_percent)\n",
    "\n",
    "print('Training : Validation : Testing ratio set to', int(train_percent*100), \":\", int(valid_percent*100), \":\", int(test_percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Kaggle API to download data\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "if competition+\".zip\" in os.listdir(download_path):\n",
    "    print(\"***Dataset present at local path. Skipping download***\")\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    api.competition_download_files(\"dogs-vs-cats\",path=download_path)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Data \n",
    "\n",
    "print(\"Extracting dataset...\")\n",
    "dataset = zipfile.ZipFile(download_path+competition+\".zip\", 'r')\n",
    "dataset.extractall(path=download_path)\n",
    "print(len(dataset.namelist()),\"files extracted \\n\")\n",
    "\n",
    "train_dir = download_path+\"train/\"\n",
    "print(\"Extracting training files...\")\n",
    "trainZip = zipfile.ZipFile(download_path+\"train.zip\", 'r')\n",
    "print(len(trainZip.namelist())-1,\"training images found\") #Removing 1 from len(trainZip.namelist()) since train/ is counted\n",
    "trainZip.extractall(path=download_path)\n",
    "\n",
    "train_files = random.sample(os.listdir(train_dir),k = dataset_size)\n",
    "\n",
    "# zipfile.ZipFile(download_path+\"test1.zip\", 'r').extractall(path=download_path) # if you'd like to extract test images as well (non-labeled)\n",
    "# test_files = os.listdir(download_path+\"test1\")\n",
    "\n",
    "# Cleaning up\n",
    "for file in dataset.filelist:\n",
    "    os.remove(file.filename)\n",
    "# os.remove(dataset.filename) if you'd like to remove the original zip file from Kaggle as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining augmentations\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T10:27:42.730340Z",
     "iopub.status.busy": "2021-09-16T10:27:42.730064Z",
     "iopub.status.idle": "2021-09-16T10:27:42.742933Z",
     "shell.execute_reply": "2021-09-16T10:27:42.742203Z",
     "shell.execute_reply.started": "2021-09-16T10:27:42.730292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "\n",
    "class DogsVsCatsDataset(Dataset):\n",
    "    def __init__(self, file_list, dir, mode='train', transform = val_transform):\n",
    "        self.file_list = file_list\n",
    "        self.dir = dir\n",
    "        self.mode= mode\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = PIL.Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
    "        img = self.transform(img)\n",
    "        img = np.array(img)\n",
    "        if 'dog' in self.file_list[idx]:\n",
    "            self.label = 1\n",
    "        else:\n",
    "            self.label = 0\n",
    "        return img.astype('float32'), self.label\n",
    "\n",
    "\n",
    "train_files, test_files = train_test_split(train_files, \n",
    "                                    test_size=train_val_test_split[2], \n",
    "                                    random_state=42\n",
    "                                    )\n",
    "train_files, valid_files = train_test_split(train_files,\n",
    "                                    test_size=train_val_test_split[1]/train_val_test_split[0], \n",
    "                                    random_state=42\n",
    "                                    )\n",
    "\n",
    "TrainDataSet = DogsVsCatsDataset(train_files, dir = train_dir, mode='train', transform = train_transform)\n",
    "TrainDataLoader = DataLoader(TrainDataSet, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "ValidDataSet = DogsVsCatsDataset(valid_files, dir = train_dir, mode='valid')\n",
    "ValidDataLoader = DataLoader(ValidDataSet, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "TestDataSet = DogsVsCatsDataset(test_files, dir = train_dir, mode='test')\n",
    "TestDataLoader = DataLoader(TestDataSet, batch_size = batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T10:27:44.514195Z",
     "iopub.status.busy": "2021-09-16T10:27:44.513899Z",
     "iopub.status.idle": "2021-09-16T10:27:45.771043Z",
     "shell.execute_reply": "2021-09-16T10:27:45.769969Z",
     "shell.execute_reply.started": "2021-09-16T10:27:44.514147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualising to ensure augmentions/transformations done without errors\n",
    "\n",
    "samples, labels = iter(TrainDataLoader).next()\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
    "np_grid_imgs = grid_imgs.numpy()\n",
    "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model and training device\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = torchvision.models.densenet121(pretrained=True,drop_rate=drop_rate)\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 500),\n",
    "    nn.Linear(500, 2)\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch\",epoch+1,\"/\",epochs)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    itr = 1\n",
    "    tot_itr = len(TrainDataLoader)\n",
    "    for samples, labels in tqdm.tqdm(TrainDataLoader, desc = \"Training\", unit = \" Iterations\"):\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(samples)\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        train_acc+= torch.mean(correct.float())\n",
    "        torch.cuda.empty_cache()\n",
    "        itr += 1\n",
    "        \n",
    "    train_loss_list.append(train_loss/tot_itr)\n",
    "    train_acc_list.append(train_acc.item()/tot_itr)\n",
    "    print(' Total Loss: {:.4f}, Accuracy: {:.1f} %'.format(train_loss, train_acc/tot_itr*100))\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss=0\n",
    "    valid_acc=0\n",
    "    itr=1\n",
    "    tot_itr = len(ValidDataLoader)\n",
    "    for samples, labels in tqdm.tqdm(ValidDataLoader, desc = \"Validating\", unit = \" Iterations\"):\n",
    "        with torch.no_grad():\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "            output = model(samples)\n",
    "            loss = criterion(output, labels)\n",
    "            valid_loss += loss.item()\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            valid_acc += torch.mean(correct.float())\n",
    "            torch.cuda.empty_cache()\n",
    "            itr += 1\n",
    "            \n",
    "    valid_loss_list.append(valid_loss/tot_itr)\n",
    "    valid_acc_list.append(valid_acc.item()/tot_itr)\n",
    "    print('-----------------------------> Validation Loss: {:.4f}, Accuracy: {:.1f} %'.format(valid_loss, valid_acc/tot_itr*100))\n",
    "\n",
    "test_acc = 0\n",
    "itr = 1\n",
    "tot_itr = len(TestDataLoader)\n",
    "for samples, labels in tqdm.tqdm(TestDataLoader, desc = \"Testing\", unit = \" Iterations\"):\n",
    "        with torch.no_grad():\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "            output = model(samples)\n",
    "            loss = criterion(output, labels)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            test_acc += torch.mean(correct.float())\n",
    "            torch.cuda.empty_cache()\n",
    "            itr += 1\n",
    "print('-----------------------------> Testing Accuracy: {:.1f} %'.format(test_acc/tot_itr*100))\n",
    "\n",
    "plt.plot(train_loss_list, label='loss')\n",
    "plt.plot(train_acc_list, label='accuracy')\n",
    "plt.legend()\n",
    "plt.title('training loss and accuracy')\n",
    "plt.tick_params(axis='x', colors='red')\n",
    "plt.tick_params(axis='y', colors='red')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(valid_loss_list, label='loss')\n",
    "plt.plot(valid_acc_list, label='accuracy')\n",
    "plt.legend()\n",
    "plt.title('validation loss and accuracy')\n",
    "plt.tick_params(axis='x', colors='red')\n",
    "plt.tick_params(axis='y', colors='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
